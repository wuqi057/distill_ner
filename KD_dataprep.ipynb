{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Load teacher model, news data and conll data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "path = ''\n",
                "import flair\n",
                "print(flair.__version__)\n",
                "\n",
                "from flair.models import SequenceTagger\n",
                "teacher = SequenceTagger.load(\"flair/ner-german-large\")\n",
                "\n",
                "import json\n",
                "import numpy as np\n",
                "sentences_news = json.load(open(path+'data/news_12.5k.json', 'r', encoding='utf-8'))\n",
                "sentences_news = [sent for sent in sentences_news if sent != ''] # filter out empty sentences\n",
                "\n",
                "print(np.histogram([len(s) for s in sentences_news]))\n",
                "\n",
                "from flair.datasets import CONLL_03_GERMAN\n",
                "corpus = CONLL_03_GERMAN(base_path = path+'data/' ,encoding= 'latin-1' )\n",
                "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
                "\n",
                "print('len(news data)',len(sentences_news))\n",
                "print('len(conll corpus train)',len(corpus.train))\n",
                "print('len(conll corpus test',len(corpus.test))\n",
                "print('len(conll corpus dev)',len(corpus.dev))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0.8.0.post1\n",
                        "2021-08-20 10:55:12,860 loading file /Users/Wu/.flair/models/ner-german-large/6b8de9edd73722050be2547acf64c037b2df833c6e8f0e88934de08385e26c1e.4b0797effcc6ebb1889d5d29784b97f0a099c1569b319d87d7c387e44e2bba48\n",
                        "(array([1122, 3387, 3433, 2387, 1239,  571,  290,  149,   77,   49]), array([  1. ,  45.8,  90.6, 135.4, 180.2, 225. , 269.8, 314.6, 359.4,\n",
                        "       404.2, 449. ]))\n",
                        "2021-08-20 10:55:31,146 Reading data from data/conll_03_german\n",
                        "2021-08-20 10:55:31,148 Train: data/conll_03_german/deu.train\n",
                        "2021-08-20 10:55:31,148 Dev: data/conll_03_german/deu.dev\n",
                        "2021-08-20 10:55:31,149 Test: data/conll_03_german/deu.testb\n",
                        "len(news data) 12704\n",
                        "len(conll corpus train) 12705\n",
                        "len(conll corpus test 3160\n",
                        "len(conll corpus dev) 93\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# prepare logits results and get output --- conll data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "source": [
                "from flair.data import Sentence\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "\n",
                "sentences_conll = []\n",
                "for sent in corpus.train:\n",
                "    sentences_conll.append(sent.to_original_text())\n",
                "\n",
                "logits = []\n",
                "for sent in tqdm(sentences_conll):\n",
                "    sent = Sentence(sent) \n",
                "    feature = teacher.forward([sent]).detach() #NOTE: forward function takes batch as input, here seen as batch_size=1, add extra list dim.\n",
                "    logits.append(feature.squeeze(0)) #NOTE: squeeze the additional 1 dim caused by batch_size 1, but not the additional 1 dim caused by len(sentence) = 1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 12705/12705 [1:13:26<00:00,  2.88it/s]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "from data_KD import Sentence\n",
                "data_wolabel = []\n",
                "for sent,l in zip(sentences_conll,logits):\n",
                "    data_wolabel.append(Sentence(sent,logits=l))\n",
                "\n",
                "data_wlabel = corpus.train\n",
                "\n",
                "for sent_wl,sent_wol in zip(data_wlabel ,data_wolabel):\n",
                "    for token_wl,token_wol in zip(sent_wl,sent_wol):\n",
                "        tag = token_wl.get_tag('ner')\n",
                "        token_wol.add_tag_label('ner',tag)\n",
                "\n",
                "import pickle\n",
                "with open(path+'data/data_conll_LogitsLabel_12.7k.pickle', 'wb') as handle:\n",
                "    pickle.dump(data_wolabel, handle, protocol=pickle.HIGHEST_PROTOCOL)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "data_wolabel [13].get_spans('ner')"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[<MISC-span (5): \"deutscher\">, <ORG-span (10,11): \"Roten Armee\">]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 57
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# prepare logits results and get output --- news data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import pickle \n",
                "from flair.data import Sentence\n",
                "from tqdm import tqdm\n",
                "\n",
                "logits = []\n",
                "\n",
                "for sent in tqdm(sentences_news):\n",
                "    sent = Sentence(sent) \n",
                "    feature = teacher.forward([sent]).detach() #NOTE: forward function takes batch as input, here seen as batch_size=1, add extra list dim.\n",
                "    logits.append(feature.squeeze(0)) #NOTE: squeeze the additional 1 dim caused by batch_size 1, but not the additional 1 dim caused by len(sentence) = 1\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 12704/12704 [1:45:28<00:00,  2.01it/s]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "from data_KD import Sentence\n",
                "data_wolabel = []\n",
                "\n",
                "for sent,l in zip(sentences_news,logits):\n",
                "    data_wolabel.append(Sentence(sent,logits=l))\n",
                "\n",
                "for sent in data_wolabel:\n",
                "    tags,all_tags= teacher._obtain_labels(\n",
                "        feature=sent.logits.unsqueeze(0),\n",
                "        batch_sentences=[sent],\n",
                "        transitions=None, # self.use_crf = False with teacher model\n",
                "        get_all_tags=False,\n",
                "    )\n",
                "    for token,tag in zip(sent.tokens,tags[0]):\n",
                "        token.add_tag_label('ner',tag)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "with open(path+'data/data_news_LogitsLabel_12.7k.pickle', 'wb') as handle:\n",
                "    pickle.dump(data_wolabel, handle, protocol=pickle.HIGHEST_PROTOCOL)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}